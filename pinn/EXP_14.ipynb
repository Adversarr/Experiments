{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% solve the Helmoltz equation with Hard-constrain DNN\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from scipy.stats import qmc\n",
    "from scipy import integrate\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "import random\n",
    "import os\n",
    "from tqdm import trange\n",
    "import csv\n",
    "\n",
    "## Environment preparing\n",
    "device = torch.device('cpu')\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "\n",
    "torch.set_default_dtype(torch.float64)\n",
    "\n",
    "plt.rcParams['figure.figsize'] = [6.4, 4.8]\n",
    "plt.rcParams['figure.dpi'] = 100\n",
    "\n",
    "seed = 1234\n",
    "random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% solve the Helmoltz equation with Hard-constrain DNN\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from scipy.stats import qmc\n",
    "from scipy import integrate\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "import random\n",
    "import os\n",
    "from tqdm import trange\n",
    "import csv\n",
    "\n",
    "## Environment preparing\n",
    "device = torch.device('cpu')\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "\n",
    "torch.set_default_dtype(torch.float64)\n",
    "\n",
    "plt.rcParams['figure.figsize'] = [6.4, 4.8]\n",
    "plt.rcParams['figure.dpi'] = 100\n",
    "\n",
    "seed = 1234\n",
    "random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% PINN network\n",
    "class PINN(nn.Module):\n",
    "    def __init__(self,in_dim, h_dim, out_dim):\n",
    "        super(PINN, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(in_dim,h_dim),\n",
    "            nn.SiLU(),\n",
    "            #torch.nn.BatchNorm1d(30, momentum=0.5),\n",
    "            #nn.Softplus(beta=1, threshold=20),\n",
    "            nn.Linear(h_dim,h_dim),\n",
    "            nn.SiLU(),\n",
    "            #torch.nn.BatchNorm1d(30, momentum=0.5),\n",
    "            # nn.Softplus(beta=1, threshold=20),\n",
    "            nn.Linear(h_dim,h_dim),\n",
    "            nn.SiLU(),\n",
    "            #torch.nn.BatchNorm1d(30, momentum=0.5),\n",
    "            #nn.Softplus(beta=1, threshold=20),\n",
    "            nn.Linear(h_dim, h_dim),\n",
    "            nn.SiLU(),\n",
    "            #torch.nn.BatchNorm1d(30, momentum=0.5),\n",
    "            #nn.Softplus(beta=1, threshold=20),\n",
    "            nn.Linear(h_dim, h_dim),\n",
    "            nn.SiLU(),\n",
    "            #torch.nn.BatchNorm1d(30, momentum=0.5),\n",
    "            #nn.Softplus(beta=1, threshold=20),\n",
    "            nn.Linear(h_dim, h_dim),\n",
    "            nn.SiLU(),\n",
    "            #torch.nn.BatchNorm1d(30, momentum=0.5),\n",
    "            #nn.Softplus(beta=1, threshold=20),\n",
    "            nn.Linear(h_dim, h_dim),\n",
    "            nn.SiLU(),\n",
    "            #torch.nn.BatchNorm1d(30, momentum=0.5),\n",
    "            #nn.Softplus(beta=1, threshold=20),\n",
    "            nn.Linear(h_dim, out_dim),\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        out = self.model(x)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "## define loss function\n",
    "# loss for Neuman boundary constraint\n",
    "def loss_Neuman(model_pr,X_bd,Normal_ij,g_ij,be_ij,grad_be_ij,grad_al_ij):\n",
    "    # X_ij = [vec(r),vec(z)] in tensor[N,2]\n",
    "    # Normal_ij means the normal vector of boundary\n",
    "    # pn_ij: true value in boundary; be_ij/grad_be_ij/grad_al_ij: constuct function/its grad\n",
    "    y_ij_r = model_pr(X_bd)\n",
    "    print(X_bd.shape)\n",
    "    y_ij_grad_r = torch.autograd.grad(\n",
    "        y_ij_r,X_bd,\n",
    "        grad_outputs=torch.ones_like(y_ij_r),\n",
    "        retain_graph=True,\n",
    "        create_graph=True\n",
    "    )[0]\n",
    "    yr_ij_r = y_ij_grad_r[:,0:1]\n",
    "    yz_ij_r = y_ij_grad_r[:,1:2]\n",
    "\n",
    "    normal_r = Normal_ij[:,0:1]\n",
    "    normal_z = Normal_ij[:,1:2]\n",
    "    print(Normal_ij.shape)\n",
    "\n",
    "    print(g_ij.shape)\n",
    "\n",
    "    tilde_p_ne = normal_r*((yr_ij_r*be_ij + grad_be_ij[:,0:1]*y_ij_r)+grad_al_ij[:,0:1]) + \\\n",
    "        normal_z*((yz_ij_r*be_ij + grad_be_ij[:,1:2]) + grad_al_ij[:,1:2])\n",
    "    loss_Ne = torch.mean((tilde_p_ne-g_ij)**2)\n",
    "    \n",
    "    return loss_Ne\n",
    "\n",
    "# loss for data_res constraint\n",
    "def loss_res_p(model_pr,X_ij,t_p_ij,al_ij,be_ij):\n",
    "    # t_p_ij means the true value of p\n",
    "    p_ij_r = model_pr(X_ij)\n",
    "    loss_res = torch.mean((t_p_ij - (p_ij_r*be_ij+al_ij))**2)\n",
    "    return loss_res\n",
    "\n",
    "def loss_equation(model_pr,X_ij,al_ij,be_ij,k,q_ij,\\\n",
    "                  grad_al_ij,grad_be_ij,grad_grad_al_ij,grad_grad_be_ij):\n",
    "    # assume grad_grad_al_ij -> a_rr a_rz a_zr a_zz\n",
    "    # q_ij is Non-homogeneous term\n",
    "    y_ij_r = model_pr(X_ij)\n",
    "\n",
    "    y_ij_grad_r = torch.autograd.grad(\n",
    "        y_ij_r,X_ij,\n",
    "        grad_outputs=torch.ones_like(y_ij_r),\n",
    "        retain_graph=True,\n",
    "        create_graph=True\n",
    "    )[0] # yre_r yre_z\n",
    "\n",
    "    yr_ij_r = y_ij_grad_r[:,0:1]\n",
    "    yz_ij_r = y_ij_grad_r[:,1:2]\n",
    "    y_drdX_r = torch.autograd.grad(\n",
    "        yr_ij_r,X_ij,\n",
    "        grad_outputs=torch.ones_like(yr_ij_r),\n",
    "        retain_graph=True,\n",
    "        create_graph=True\n",
    "    )[0] # y_rr y_rz\n",
    "    y_dzdX_r = torch.autograd.grad(\n",
    "        yz_ij_r,X_ij,\n",
    "        grad_outputs=torch.ones_like(yz_ij_r),\n",
    "        retain_graph=True,\n",
    "        create_graph=True\n",
    "    )[0] # y_zr y_zz\n",
    "    yrr_ij_r = y_drdX_r[:,0:1] # y_rr_r\n",
    "    yzz_ij_r = y_dzdX_r[:,1:2] # y_zz_r\n",
    "    # print(yrr_ij_r)\n",
    "    tilde_p_rr = grad_grad_al_ij[:,0:1] + 2*grad_be_ij[:,0:1]*yr_ij_r + \\\n",
    "        grad_grad_be_ij[:,0:1]*y_ij_r + yrr_ij_r*be_ij # a_rr + 2b_ry_r + b_rr + y_rr\n",
    "    tilde_p_r = 1/X_ij[:,0:1]*(grad_al_ij[:,0:1] + grad_be_ij[:,0:1]*y_ij_r + be_ij*yr_ij_r) # (a_r + b_r y + y_r b)/r\n",
    "    tilde_p_zz = grad_grad_al_ij[:,1:2] + 2*grad_be_ij[:,1:2]*yz_ij_r + \\\n",
    "        grad_grad_be_ij[:,1:2]*y_ij_r + yzz_ij_r*be_ij     # a_zz + 2b_z y_z + y_zz b + b_zz y\n",
    "    tilde_p = al_ij + be_ij*y_ij_r\n",
    "    loss_eq = torch.mean((tilde_p_rr + tilde_p_r + tilde_p_zz + k**2*tilde_p - q_ij)**2)\n",
    "    return loss_eq\n",
    "\n",
    "def loss_total_p(model_pr,X_ij,X_bound,t_p_ij,al_ij,be_ij,Normal_bound,g_ij,lambda_k,\\\n",
    "                 k,q_ij,grad_al_ij,grad_be_ij,grad_grad_al_ij,grad_grad_be_ij,\\\n",
    "                 g_bound,be_bound,grad_be_bound,grad_al_bound):\n",
    "    # lambda_k: penalty issue\n",
    "    loss_res = loss_res_p(model_pr,X_ij,t_p_ij,al_ij,be_ij)\n",
    "    loss_Ne = loss_Neuman(model_pr,X_bound,Normal_bound,g_bound,be_bound,grad_be_bound,grad_al_bound)\n",
    "    loss_eq = loss_equation(model_pr,X_ij,al_ij,be_ij,k,q_ij,\\\n",
    "                  grad_al_ij,grad_be_ij,grad_grad_al_ij,grad_grad_be_ij)\n",
    "    loss_total = loss_eq + loss_res + loss_Ne/3\n",
    "    return loss_total,loss_Ne,loss_res,loss_eq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## define g,h,alpha(al),beta(be),q and its grad\n",
    "p0 = 0.5   # reference pressure\n",
    "H = 100     # heigh of sea\n",
    "zs = 20\n",
    "c = 1500\n",
    "f = 300\n",
    "k = f*2*np.pi/c\n",
    "# the function using now has maximum value f(0.377H) = 0.052H^4\n",
    "def al(r,z,k):\n",
    "    al_ij = p0*z**2*(2*z - 3*H)/H**3 \n",
    "    # print(al_ij)\n",
    "    return al_ij\n",
    "\n",
    "def be(r,z,k):\n",
    "    be_ij = p0*z**2*(2*z - 3*H)/H**3\n",
    "    return be_ij\n",
    "\n",
    "def grad_al(r,z,k):\n",
    "    al_r = 0.0 * r\n",
    "    al_z = p0 * (6*z**2 - 6*z*H)/H**3\n",
    "    # grad_al_ij = torch.empty(list(r.size())[0],2)\n",
    "    # grad_al_ij[:,0] = 0.0*r\n",
    "    # grad_al_ij[:,1] = p0*(z-H)\n",
    "    return np.hstack([al_r,al_z])\n",
    "\n",
    "def grad_be(r,z,k):\n",
    "    be_r = 0.0 * r\n",
    "    be_z = p0 * (6*z**2 - 6*z*H)/H**3\n",
    "    # grad_be_ij = torch.empty(list(r.size())[0],2)\n",
    "    # grad_be_ij[:,0] = 0.0*r\n",
    "    # grad_be_ij[:,1] = p0*(z-H)\n",
    "    return np.hstack([be_r,be_z])\n",
    "\n",
    "def grad_grad_al(r,z,k):\n",
    "    al_rr = 0.0 * r\n",
    "    al_zz = p0 * (12*z - 6*H)/H**3\n",
    "    # grad_grad_al_ij = torch.empty(list(r.size())[0],4)\n",
    "    # grad_grad_al_ij[:,0] = 0.0*r\n",
    "    # grad_grad_al_ij[:,1] = 0.0*r\n",
    "    # grad_grad_al_ij[:,2] = 0.0*r\n",
    "    # grad_grad_al_ij[:,3] = p0\n",
    "    return np.hstack([al_rr, al_zz])\n",
    "\n",
    "def grad_grad_be(r,z,k):\n",
    "    be_rr = 0.0 * r\n",
    "    be_zz = p0 * (12*z - 6*H)/H**3\n",
    "    # grad_grad_be_ij = torch.empty(list(r.size())[0],4)\n",
    "    # grad_grad_be_ij[:,0] = 0.0*r\n",
    "    # grad_grad_be_ij[:,1] = 0.0*r\n",
    "    # grad_grad_be_ij[:,2] = 0.0*r\n",
    "    # grad_grad_be_ij[:,3] = p0\n",
    "    return np.hstack([be_rr, be_zz])\n",
    "\n",
    "def Normal_vec(r,z,k):\n",
    "    # for flat bottom\n",
    "    # Normal_vec_ij = torch.empty(list(r.size())[0],2)\n",
    "    # Normal_vec_ij[:,0] = 0.0*r\n",
    "    # Normal_vec_ij[:,1] = 1.0*r\n",
    "    N_r = 0.0 * r\n",
    "    N_z = 1.0 * np.ones_like(r)\n",
    "    # for clien bottom (0,H) (R,B)\n",
    "    # R = 1000\n",
    "    # B = 10\n",
    "    # Normal_vec_ij[:,0] = -B/R*r\n",
    "    # Normal_vec_ij[:,1] = 1.0*r\n",
    "    return np.hstack([N_r,N_z])\n",
    "\n",
    "def q(r,z,k):\n",
    "    # delta(z-zs)delta(r)\n",
    "    q_ij = 0.0 * r\n",
    "    q_ij[r == 0 & z == zs] = 1\n",
    "    return q_ij"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  1.00502515]\n",
      " [  2.0100503 ]\n",
      " [  3.01507544]\n",
      " ...\n",
      " [197.9899445 ]\n",
      " [198.9949799 ]\n",
      " [200.        ]]\n",
      "[[ 7.0500001e-15]\n",
      " [-5.5100001e-15]\n",
      " [-4.4700000e-15]\n",
      " ...\n",
      " [-2.6805490e-02]\n",
      " [-4.7217552e-03]\n",
      " [ 2.3267288e-02]]\n"
     ]
    }
   ],
   "source": [
    "#%% initial training data using K\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import random\n",
    "\n",
    "random.seed()\n",
    "\n",
    "# read data from csv,z in[1001,1] r in [1000,1] p in [1001,1000]\n",
    "r_d = pd.read_csv('data/K_300_r.csv')['r']\n",
    "r_d = torch.tensor(r_d).reshape(-1,1)\n",
    "z_d = pd.read_csv('data/K_300_z.csv')['z']\n",
    "z_d = torch.tensor(z_d).reshape(-1,1)\n",
    "l = list(np.linspace(0, 1, 200))\n",
    "p_d_i = pd.read_csv('data/K_300_pi.csv',names=l)[l].values\n",
    "p_d_r = pd.read_csv('data/K_300_pr.csv',names=l)[l].values\n",
    "p_d_r = p_d_r.astype(np.float32)\n",
    "p_d_i = p_d_i.astype(np.float32)\n",
    "#p_d_r = np.transpose(p_d_r)\n",
    "#p_d_i = np.transpose(p_d_i)\n",
    "\n",
    "# Density of grid points(inner)\n",
    "# N_inner = 2000\n",
    "# random_r = list(np.random.randint(1,999,N_inner)) # get points inner, except z = H/0\n",
    "# random_z = list(np.random.randint(1,999,N_inner))\n",
    "# r = r_d[random_r,0:1].numpy()\n",
    "# z = z_d[random_z,0:1].numpy()\n",
    "r,z = np.meshgrid(r_d[1:200], z_d[0:100])\n",
    "r = r.reshape(100*199,1)\n",
    "z = z.reshape(100*199,1)\n",
    "X_ij = np.hstack([r,z]) # [[r[0:199],0],[r[0:199],1],...,]\n",
    "X_ij = torch.from_numpy(X_ij).requires_grad_(True).double().to(device)\n",
    "# r = X_ij[:,0:1]\n",
    "# z = X_ij[:,1:2]\n",
    "\n",
    "# list_z = list(np.linspace(0,99,100))\n",
    "# Get pressure data\n",
    "pd_r_ij = p_d_r[0:100,1:200]\n",
    "pd_i_ij = p_d_i[0:100,1:200]\n",
    "pd_r_ij = pd_r_ij.reshape(100*199,1)\n",
    "pd_i_ij = pd_i_ij.reshape(100*199,1)\n",
    "print(r)\n",
    "print(pd_r_ij)\n",
    "pd_r_ij = torch.from_numpy(pd_r_ij).double().to(device)\n",
    "pd_i_ij = torch.from_numpy(pd_i_ij).double().to(device)\n",
    "pd_ij = torch.complex(pd_r_ij,pd_i_ij).to(device)\n",
    "# print(p_d_i[0,1])\n",
    "# pd_ij = np.hstack(pd_r_ij+1j*pd_i_ij)\n",
    "# pd_r_ij = pd_ij[:,0:1]\n",
    "# pd_i_ij = pd_ij[:,1:2]\n",
    "# x = torch.complex(torch.tensor([1.0, 2.0]), torch.tensor([3.0, 4.0]))\n",
    "\n",
    "# Get boundary data\n",
    "N_bound = 100\n",
    "random_r_bo = random.sample(range(0,199), N_bound)\n",
    "random_z_bo = 100*np.ones(N_bound)\n",
    "r_bound = r_d[random_r_bo,0:1].numpy()\n",
    "z_bound = z_d[random_z_bo,0:1].numpy()\n",
    "X_bound = np.hstack([r_bound,z_bound])\n",
    "X_bound = torch.from_numpy(X_bound).requires_grad_(True).double().to(device)\n",
    "# Get boundary pressure\n",
    "\n",
    "# Get other funtions' value/grad\n",
    "al_ij = al(r,z,k)\n",
    "be_ij = be(r,z,k)\n",
    "grad_al_ij = grad_al(r,z,k)\n",
    "grad_be_ij = grad_be(r,z,k)\n",
    "grad_grad_al_ij = grad_grad_al(r,z,k)\n",
    "grad_grad_be_ij = grad_grad_be(r,z,k)\n",
    "norm_vec_bound = Normal_vec(r_bound,z_bound,k)\n",
    "g_bound = 0.0 * r_bound\n",
    "be_bound = be(r_bound,z_bound,k)\n",
    "grad_be_bound = grad_be(r_bound,z_bound,k)\n",
    "grad_al_bound = grad_al(r_bound,z_bound,k)\n",
    "q_ij = 0.0 * r\n",
    "g_ij = 0.0 * r\n",
    "# q_ij[z==zs & r == 0] = 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(19900, 1)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g_ij.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# translate to tensor\n",
    "al_ij = torch.from_numpy(al_ij).double().to(device)\n",
    "be_ij = torch.from_numpy(be_ij).double().to(device)\n",
    "grad_al_ij = torch.from_numpy(grad_al_ij).double().to(device)\n",
    "grad_be_ij = torch.from_numpy(grad_be_ij).double().to(device)\n",
    "grad_grad_al_ij = torch.from_numpy(grad_grad_al_ij).double().to(device)\n",
    "grad_grad_be_ij = torch.from_numpy(grad_grad_be_ij).double().to(device)\n",
    "norm_vec_bound = torch.from_numpy(norm_vec_bound).double().to(device)\n",
    "q_ij = torch.from_numpy(q_ij).double().to(device)\n",
    "g_ij = torch.from_numpy(g_ij).double().to(device)\n",
    "g_bound = torch.from_numpy(g_bound).double().to(device)\n",
    "be_bound = torch.from_numpy(be_bound).double().to(device)\n",
    "grad_be_bound = torch.from_numpy(grad_be_bound).double().to(device)\n",
    "grad_al_bound = torch.from_numpy(grad_al_bound).double().to(device)\n",
    "# translate to complex\n",
    "# al_ij = torch.complex(al_ij,torch.zeros_like(al_ij))\n",
    "# be_ij = torch.complex(be_ij,torch.zeros_like(be_ij))\n",
    "# grad_al_ij = torch.complex(grad_al_ij,torch.zeros_like(grad_al_ij))\n",
    "# grad_be_ij = torch.complex(grad_be_ij,torch.zeros_like(grad_be_ij))\n",
    "# grad_grad_be_ij = torch.complex(grad_grad_be_ij,torch.zeros_like(grad_grad_be_ij))\n",
    "# grad_grad_al_ij = torch.complex(grad_grad_al_ij,torch.zeros_like(grad_grad_al_ij))\n",
    "# q_ij = torch.complex(q_ij,torch.zeros_like(q_ij))\n",
    "# g_ij = torch.complex(g_ij,torch.zeros_like(g_ij))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PINN(\n",
      "  (model): Sequential(\n",
      "    (0): Linear(in_features=2, out_features=64, bias=True)\n",
      "    (1): SiLU()\n",
      "    (2): Linear(in_features=64, out_features=64, bias=True)\n",
      "    (3): SiLU()\n",
      "    (4): Linear(in_features=64, out_features=64, bias=True)\n",
      "    (5): SiLU()\n",
      "    (6): Linear(in_features=64, out_features=64, bias=True)\n",
      "    (7): SiLU()\n",
      "    (8): Linear(in_features=64, out_features=64, bias=True)\n",
      "    (9): SiLU()\n",
      "    (10): Linear(in_features=64, out_features=64, bias=True)\n",
      "    (11): SiLU()\n",
      "    (12): Linear(in_features=64, out_features=64, bias=True)\n",
      "    (13): SiLU()\n",
      "    (14): Linear(in_features=64, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "PINN(\n",
      "  (model): Sequential(\n",
      "    (0): Linear(in_features=2, out_features=64, bias=True)\n",
      "    (1): SiLU()\n",
      "    (2): Linear(in_features=64, out_features=64, bias=True)\n",
      "    (3): SiLU()\n",
      "    (4): Linear(in_features=64, out_features=64, bias=True)\n",
      "    (5): SiLU()\n",
      "    (6): Linear(in_features=64, out_features=64, bias=True)\n",
      "    (7): SiLU()\n",
      "    (8): Linear(in_features=64, out_features=64, bias=True)\n",
      "    (9): SiLU()\n",
      "    (10): Linear(in_features=64, out_features=64, bias=True)\n",
      "    (11): SiLU()\n",
      "    (12): Linear(in_features=64, out_features=64, bias=True)\n",
      "    (13): SiLU()\n",
      "    (14): Linear(in_features=64, out_features=1, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Model, opt, learning rates, palnaty\n",
    "\n",
    "model_pr = PINN(2,64,1).to(device)\n",
    "model_pi = PINN(2,64,1).to(device)\n",
    "print(model_pr)\n",
    "print(model_pi)\n",
    "\n",
    "# model_p.model1.weight.grad = torch.complex(model_p.model1.weight.grad, torch.zeros_like(model_p.model1.weight.grad))\n",
    "# model_p.model1.bias.grad = torch.complex(model_p.model1.bias.grad, torch.zeros_like(model_p.model1.bias.grad))\n",
    "\n",
    "optimizer_r = torch.optim.Adam(\n",
    "    list(model_pr.parameters()),\n",
    "    lr = 0.001\n",
    ")\n",
    "\n",
    "optimizer_i = torch.optim.Adam(\n",
    "    list(model_pi.parameters()),\n",
    "    lr = 0.001\n",
    ")\n",
    "\n",
    "lambda_k = 1.00015\n",
    "max_iter = 2000\n",
    "def lr_adjust(step, optimizer: torch.optim.Adam):\n",
    "    update_lr_1 = 1000\n",
    "    update_lr_2 = 2000\n",
    "    update_lr_3 = 10000\n",
    "    update_lr_4 = 15000\n",
    "    if step == update_lr_1:\n",
    "        for g in optimizer.param_groups:\n",
    "            g['lr'] = 0.0005\n",
    "    if step == update_lr_2:\n",
    "        for g in optimizer.param_groups:\n",
    "            g['lr'] = 0.0001\n",
    "    if step == update_lr_3:\n",
    "        for g in optimizer.param_groups:\n",
    "            g['lr'] = 0.0001\n",
    "    if step == update_lr_4:\n",
    "        for g in optimizer.param_groups:\n",
    "            g['lr'] = 0.00003\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 15/2000 [00:12<26:47,  1.23it/s, loss=2.69770e-01, loss_Ne=4.01064e-08, loss_res=7.76985e-02, loss_eq=1.92072e-01, stepsize=1.00000e-03]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/yangjerry/Repo/pinn/EXP_14.ipynb 单元格 10\u001b[0m line \u001b[0;36m1\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/yangjerry/Repo/pinn/EXP_14.ipynb#X11sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m optimizer_r\u001b[39m.\u001b[39mzero_grad()\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/yangjerry/Repo/pinn/EXP_14.ipynb#X11sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m loss_total_r,loss_Ne_r,loss_res_r,loss_eq_r \\\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/yangjerry/Repo/pinn/EXP_14.ipynb#X11sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m       \u001b[39m=\u001b[39m loss_total_p(model_pr,X_ij,X_bound,pd_r_ij,al_ij,be_ij,norm_vec_bound,g_ij,lambda_k,\\\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/yangjerry/Repo/pinn/EXP_14.ipynb#X11sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m              k,q_ij,grad_al_ij,grad_be_ij,grad_grad_al_ij,grad_grad_be_ij,\\\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/yangjerry/Repo/pinn/EXP_14.ipynb#X11sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m                 g_bound,be_bound,grad_be_bound,grad_al_bound)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/yangjerry/Repo/pinn/EXP_14.ipynb#X11sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m loss_total_r\u001b[39m.\u001b[39mbackward()\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/yangjerry/Repo/pinn/EXP_14.ipynb#X11sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m optimizer_r\u001b[39m.\u001b[39mstep()\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/yangjerry/Repo/pinn/EXP_14.ipynb#X11sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m \u001b[39mif\u001b[39;00m step \u001b[39m%\u001b[39m \u001b[39m100\u001b[39m \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/lib/python3.11/site-packages/torch/_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    477\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    478\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    479\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    480\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    485\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[1;32m    486\u001b[0m     )\n\u001b[0;32m--> 487\u001b[0m torch\u001b[39m.\u001b[39mautograd\u001b[39m.\u001b[39mbackward(\n\u001b[1;32m    488\u001b[0m     \u001b[39mself\u001b[39m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39minputs\n\u001b[1;32m    489\u001b[0m )\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/lib/python3.11/site-packages/torch/autograd/__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    195\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[1;32m    197\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    198\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    199\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 200\u001b[0m Variable\u001b[39m.\u001b[39m_execution_engine\u001b[39m.\u001b[39mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    201\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[1;32m    202\u001b[0m     allow_unreachable\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, accumulate_grad\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Training real part\n",
    "model_pr.train()\n",
    "\n",
    "loss_hist_r = []\n",
    "pbar = trange(max_iter)\n",
    "\n",
    "for step in pbar:\n",
    "    lr_adjust(step, optimizer_r)\n",
    "    optimizer_r.zero_grad()\n",
    "\n",
    "    loss_total_r,loss_Ne_r,loss_res_r,loss_eq_r \\\n",
    "          = loss_total_p(model_pr,X_ij,X_bound,pd_r_ij,al_ij,be_ij,norm_vec_bound,g_ij,lambda_k,\\\n",
    "                 k,q_ij,grad_al_ij,grad_be_ij,grad_grad_al_ij,grad_grad_be_ij,\\\n",
    "                    g_bound,be_bound,grad_be_bound,grad_al_bound)\n",
    "    loss_total_r.backward()\n",
    "    optimizer_r.step()\n",
    "    \n",
    "    if step % 100 == 0:\n",
    "        pbar.set_postfix({\n",
    "            'loss': '{0:.5e}'.format(loss_total_r.item()),\n",
    "                          'loss_Ne': '{0:.5e}'.format(loss_Ne_r.item()),\n",
    "                          'loss_res': '{0:.5e}'.format(loss_res_r.item()),\n",
    "                          'loss_eq': '{0:.5e}'.format(loss_eq_r.item()),\n",
    "                          'stepsize': '{0:.5e}'.format(optimizer_r.param_groups[0]['lr'])\n",
    "        })\n",
    "        loss_hist_r.append(loss_total_r.item())\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.plot(loss_hist_r, 'b')\n",
    "ax.set_yscale('log')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([100, 2])\n",
      "torch.Size([100, 2])\n",
      "torch.Size([100, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  1.18it/s, loss=2.22669e-01, loss_Ne=2.45028e-07, loss_res=6.43146e-02, loss_eq=1.58354e-01, stepsize=1.00000e-03]\n"
     ]
    }
   ],
   "source": [
    "model_pi.train()\n",
    "\n",
    "loss_hist_i = []\n",
    "pbar = trange(1)\n",
    "\n",
    "for step in pbar:\n",
    "    # lr_adjust(step, optimizer_r)\n",
    "    lr_adjust(step, optimizer_i)\n",
    "    optimizer_i.zero_grad()\n",
    "\n",
    "    loss_total_i,loss_Ne_i,loss_res_i,loss_eq_i \\\n",
    "          = loss_total_p(model_pi,X_ij,X_bound,pd_i_ij,al_ij,be_ij,norm_vec_bound,g_ij,lambda_k,\\\n",
    "                 k,q_ij,grad_al_ij,grad_be_ij,grad_grad_al_ij,grad_grad_be_ij,\\\n",
    "                    g_bound,be_bound,grad_be_bound,grad_al_bound)\n",
    "    loss_total_i.backward()\n",
    "    optimizer_i.step()\n",
    "    \n",
    "    if step % 100 == 0:\n",
    "        pbar.set_postfix({\n",
    "            'loss': '{0:.5e}'.format(loss_total_i.item()),\n",
    "                          'loss_Ne': '{0:.5e}'.format(loss_Ne_i.item()),\n",
    "                          'loss_res': '{0:.5e}'.format(loss_res_i.item()),\n",
    "                          'loss_eq': '{0:.5e}'.format(loss_eq_i.item()),\n",
    "                          'stepsize': '{0:.5e}'.format(optimizer_i.param_groups[0]['lr'])\n",
    "        })\n",
    "        loss_hist_i.append(loss_total_i.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.plot(loss_hist_i, 'b')\n",
    "ax.set_yscale('log')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-0.0501, grad_fn=<MeanBackward0>)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.colorbar.Colorbar at 0x2ae939990>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "y_train_r = model_pr(X_ij)\n",
    "y_train_i = model_pi(X_ij)\n",
    "p_train_r = al_ij + be_ij*y_train_r\n",
    "p_train_i = al_ij + be_ij*y_train_i\n",
    "p_train = p_train_r + 1j*p_train_i\n",
    "\n",
    "er_train = p_train_r - pd_r_ij\n",
    "ei_train = p_train_i - pd_i_ij\n",
    "\n",
    "print(torch.mean(er_train))\n",
    "\n",
    "p = plt.pcolor((ei_train**2).detach().numpy())\n",
    "plt.colorbar(ticks = [0,0.1,0.2])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "N_test = 1000*1001\n",
    "r_mesh,z_mesh = np.meshgrid(r_d, z_d)\n",
    "r_mesh_r = r_mesh.reshape(N_test,1)\n",
    "z_mesh_r = z_mesh.reshape(N_test,1)\n",
    "X_test = np.hstack([r_mesh_r,z_mesh_r])\n",
    "X_test = torch.tensor(X_test)\n",
    "\n",
    "# get al and be\n",
    "al_test = al(r_mesh_r,z_mesh_r,k)\n",
    "be_test = be(r_mesh_r,z_mesh_r,k)\n",
    "\n",
    "y_test_r = model_pr(X_test).detach().cpu().numpy()\n",
    "y_test_i = model_pi(X_test).detach().cpu().numpy()\n",
    "p_test_r = al_test + be_test*y_test_r\n",
    "p_test_i = al_test + be_test*y_test_i\n",
    "\n",
    "# print(p_d_i.reshape(N_test,1))\n",
    "p_test = p_test_i*1j + p_test_r\n",
    "TL_test = -20*np.log10(np.abs(p_test))\n",
    "TL_test = TL_test.reshape(1000,1001)\n",
    "\n",
    "# get errors\n",
    "p_true = p_d_r + 1j*p_d_i\n",
    "TL_true = -10*np.log10(np.abs(p_true))\n",
    "error_all_TL = TL_test - TL_true\n",
    "error_all_p = p_true - p_test.reshape(1000,1001)\n",
    "error_av = np.mean(error_all_TL)\n",
    "# print(np.abs(error_all_p))\n",
    "# print(np.mean(np.abs(error_all_TL)))\n",
    "# print(error_all)\n",
    "# print(error_av)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "p = plt.pcolor(np.abs(error_all_p))\n",
    "clb = plt.colorbar(p)\n",
    "\n",
    "'''\n",
    "fig, ax = plt.subplots(subplot_kw={\"projection\": \"3d\"})\n",
    "surf_eu = ax.pcolormesh(r_d, z_d, TL_test, cmap=cm.coolwarm, linewidth=0.2)\n",
    "surf_eu.set_edgecolors(\"black\")\n",
    "ax.title.set_text('Exact control')\n",
    "fig.colorbar(surf_eu, shrink=0.5, aspect=5)\n",
    "plt.show()\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(al_test)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
